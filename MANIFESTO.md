# VORTEX Manifesto

## Why this project exists
AI infrastructure is becoming a gatekept resource. The most capable systems are sold at price points that remove independent researchers, students, and small teams from the frontier by default.

VORTEX exists to challenge that access model with an open architecture path: standardized interconnects, commodity memory, and publish-first design principles.

## The core claim
The present price of high-end AI hardware reflects market structure, not only physics.

If architecture choices prioritize open standards and memory economics, the cost profile of large inference systems can move from "institution-only" to "widely buildable".

## What we are building
VORTEX is an open architecture initiative for rack-scale inference clusters based on:
- commodity GPU compute nodes
- FPGA-mediated memory expansion
- PCIe/CXL fabric composition
- open operational interfaces

This repository publishes architecture-level prior art first, then implementation details in stages.

## Prior Art plus delayed open source
Full immediate disclosure can allow large incumbents to absorb novel techniques faster than an independent project can prototype and validate them. The strategy is therefore:

1. Publish architecture principles publicly (timestamped prior art).
2. Build and validate MVP.
3. Release implementation artifacts in controlled phases.
4. Transition stewardship toward open governance.

## Commitment
- No exclusive licensing that restricts broad use.
- No closed patent-wall strategy around core architecture ideas.
- Preference for standards and interoperability over lock-in.

## Success metric
Success is not valuation. Success is practical accessibility:

A small lab, startup, or university team should be able to deploy serious AI inference infrastructure without vendor dependency on a single proprietary stack.

## Publication record
Initial public prior art publication date: **February 22, 2026**.
